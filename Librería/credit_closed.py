import os
import glob
import pandas as pd
import hashlib
from collections import defaultdict
from datetime import datetime


def export_pickle(pickle_file, output_tc_al_corte):
    print("üì¶ Cargando archivo pickle...")
    try:
        df = pd.read_pickle(pickle_file)

        # üîÅ Asegura formato correcto en 'Fecha'
        if 'Fecha' in df.columns:
            df['Fecha'] = pd.to_datetime(df['Fecha'], errors='coerce')
            df['Fecha'] = df['Fecha'].dt.strftime('%d/%m/%Y')

        df.to_excel(output_tc_al_corte, index=False)
        print(f"‚úÖ Informaci√≥n exportada exitosamente a: {output_tc_al_corte}")
    except Exception as e:
        print(f"‚ùå Error al exportar el archivo: {e}")

def hash_file_content(file_path):
    """Returns the SHA256 hash of a file's content."""
    with open(file_path, 'rb') as f:
        return hashlib.sha256(f.read()).hexdigest()

def read_create_pickle(pickle_file, columns): 
    if os.path.exists(pickle_file):
        df_tc_closed = pd.read_pickle(pickle_file)
        print("‚úÖ Archivo con informaci√≥n cargado.")
    else:
        df_empty = pd.DataFrame(columns=columns + ['file_date', 'file_name'])
        df_empty.to_pickle(pickle_file)
        df_tc_closed = df_empty
        print("üìÅ Archivo no localizado, creando nuevo archivo.")
    return df_tc_closed

def process_closed_credit_accounts(folder_TC_al_corte, columns, open_folder):

    pickle_file = os.path.join(folder_TC_al_corte, 'pickle_database.pkl')
    df_readed = read_create_pickle(pickle_file, columns)
    # üîç Archivos registrados anteriormente (limpios sin hash)
    unique_files_raw = df_readed['file_name'].unique()
    cleaned_files = sorted([x.split('__HASH__')[0] for x in unique_files_raw])
    #print(f"cleaned_files: {cleaned_files}")
    unique_files_raw = df_readed['file_name'].unique()
    cleaned_files = [x.split('__HASH__')[0] for x in unique_files_raw]
    # Archivos esperados por mes hasta el actual
    now = datetime.now()
    year = now.year
    month = now.month
    expected = [f"{year}-{m:02d}.csv" for m in range(month, 0, -1)]
    missing = [f for f in expected if f not in cleaned_files]    

    while True:
        csv_files = glob.glob(os.path.join(folder_TC_al_corte, '*.csv'))        
        # üìÇ Archivos actualmente en la carpeta
        current_files = sorted([os.path.basename(f) for f in csv_files])
        #print(f"üìÇ Archivos actuales en carpeta: {current_files}")

        # ‚ùó Condici√≥n corregida:
        missing_still = [m for m in missing if m not in current_files]

        if missing_still:
            print("üìÅ No se detectaron todos los archivos esperados.")
            print("üìå Archivos faltantes:")
            for f in missing_still:
                print(f"  - {f}")
            
            open_folder(folder_TC_al_corte)
            input("üîÑ Carga los archivos de corte pendientes y presiona ENTER cuando termines.\n")
        else:
            print("‚úÖ Todos los archivos esperados est√°n presentes. Continuando...")
            break
    # Step 1: Build hash for all CSVs and detect internal duplicates
    hash_to_files = defaultdict(list)
    file_hashes = {}

    for csv_file in csv_files:
        file_hash = hash_file_content(csv_file)
        file_hashes[csv_file] = file_hash
        hash_to_files[file_hash].append(csv_file)
        open_folder

    # Highlight duplicates among CSV files (even with different names)
    for file_hash, files in hash_to_files.items():
        if len(files) > 1:
            print(f"\n*********\n‚ö†Ô∏è Archivos con contenido duplicado entre s√≠: {[os.path.basename(f) for f in files]}\n*********\n")

    # Extract previously recorded hashes from pickle
    existing_hashes = set(df_readed['file_name'].apply(lambda x: x.split("__HASH__")[-1]) if not df_readed.empty else [])

    updated = False
    files_processed = 0
    
    for csv_file in csv_files:
        file_hash = file_hashes[csv_file]
        if file_hash in existing_hashes:
            #print(f"üîÅ Archivo ya registrado previamente: {os.path.basename(csv_file)}")
            continue
        if len(hash_to_files[file_hash]) > 1 and hash_to_files[file_hash][0] != csv_file:
            # Skip all but the first occurrence of duplicated content
            print(f"üö´ Contenido duplicado ignorado: {os.path.basename(csv_file)}")
            continue

        try:
            df_csv_try = pd.read_csv(csv_file)
            if not all(col in df_csv_try.columns for col in columns):
                print(f"‚ùå Archivo excluido por encabezados incorrectos: {os.path.basename(csv_file)}")
                continue
        except Exception as e:
            print(f"‚ö†Ô∏è Error al leer {os.path.basename(csv_file)}: {e}")
            continue

        try:
            file_date = pd.to_datetime(df_csv_try['Fecha'].iloc[0], dayfirst=True)
        except Exception:
            file_date = pd.to_datetime(os.path.getmtime(csv_file), unit='s')
        df_csv = df_csv_try[columns].copy()
        df_csv['Fecha'] = pd.to_datetime(df_csv['Fecha'], dayfirst=True, errors='coerce')
        
        df_csv['file_date'] = file_date.strftime('%d/%m/%Y')
        df_csv['file_name'] = f"{os.path.basename(csv_file)}__HASH__{file_hash}"

        df_readed = pd.concat([df_readed, df_csv], ignore_index=True)
        existing_hashes.add(file_hash)
        updated = True
        files_processed += 1
        print(f"‚úÖ Archivo nuevo agregado: {os.path.basename(csv_file)}")

    if updated:
        df_readed['Fecha'] = pd.to_datetime(df_readed['Fecha'], dayfirst=True, errors='coerce')
        df_readed.sort_values(by='file_date', ascending=False, inplace=True)
        df_readed.to_pickle(pickle_file)
        print("üíæ Cambios guardados en el archivo pickle.")
    else:
        print("üì≠ No hay archivos nuevos para registrar.")

    #print("\nüìã Vista previa de los primeros registros:")
    #print(df_readed.head(10))

    #return df_readed